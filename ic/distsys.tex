
Understanding the three types of \textit{goal conflict} that \textit{TFMs} must eliminate lets us examine previous research with considerably less pessimism than its authors. While the papers discussed below have all pushed us towards a deeper understanding of the trade-offs involved in specific designs, the generality of their conclusions are limited by their models, and their specific reliance on direct mechanisms with informational limitations that make even calculating \textit{pareto optimal} price-levels well beyond the capabilities of the mechanisms themselves.

In academia, early attempts to model \textit{TFMs} in this fashion were Bitcoin-specific, starting with "Redesigning Bitcoin's Fee Market", which proposed using a "monopolistic auction" to stabilize miner revenue, and then Andrew Yao's "An Incentive Analysis" which showed this maximized miner revenue at scale. Basu, Easley, Oâ€™Hara, Sirer then proposed a modified Vickrey-Clarke-Groves mechanism as a better choice for maximizing the collective welfare of both users and miners, whose broadening of concerns to encompass multiple classes of participants showed awareness that efficiency mattered and that \textit{TFMs} are themselves subject to conflict over resource allocation within the broader economy! Within a decade of the invention of Bitcoin, computer science was on the cusp of seeing the underlying economic nature of the \textit{TFM} problem and realizing that \textit{pareto optimality} would be the social choice rule required to solve it.

The rise of Ethereum and the blocksize wars it unleashed pulled public attention away from proof-of-work, and computer science responded in 2021 when Tim Roughgarden\citet{roughgarden2021,roughgarden2024transaction} offered a paper that modelled Transaction Fee Mechanisms (TFMs) as two-sided auctions in which block producers are given a temporary monopoly over the production of a block and must strategically allocate a subset of transactions into it. Looking beyond Bitcoin towards the emerging landscape of competing approaches, Roughgarden attempted to generalize the limitations developers were experiencing in all of them into an abstract type of mechanism whose limitations could be theoretically analyzed and universally understood. As a result, Roughgarden was the first to highlight the difficulty of achieving incentive compatibility for both users (UIC) and block producers or miners (MIC) across multiple classes of mechanisms, leading to seminal works~\cite {roughgarden2021,roughgarden2024transaction} on the limitations of Bitcoin's "first-price auction" and Ethereum's EIP-1559~\cite{buterin2019eip} among others.~\cite{roughgarden2021,roughgarden2024transaction}.

Unfortunately, Roughgarden's attempt to generalize pulled attention away from questions of economic efficiency and reinforced a methodological focus on direct mechanisms and attacks involving \textit{strategic manipulation}. As a result, since 2021, the vast majority of academics working on \textit{TFM design} have followed Roughgarden in modelling \textit{TFMs} as two-party auctions in which producers clash with users over how to price blockspace. The attractiveness of the approach is obvious: it focuses on mechanism-imposed rather than external motivations for conflict, it targets an essential step in the formation of consensus, it avoids the complication of modelling the diffuse utility provided by public goods, and it uses a two-sided game that is tractable to model mathematically. As a bonus, Myerson's lemma and virtual valuations can also be used to generalize the rational strategies of participants in these games so they can be asserted to hold in larger games with many players, allowing for the generalization of conclusions to the market setting and the implication that these models are more powerful than their limitations.

Since one of the purposes of this paper is to present a mechanism that evades these problems, it is useful show how this methodological approach makes achieving \textit{pareto optimality} theoretically impossible. In this light, the first problem is the treatment of UIC and MIC as properties which can exist outside the context of a social choice rule. Instead of identifying an equilibrium like pareto optimality that guarantee both fee-optimality and collusion-resistance for users and producers alike, and asking what private information participants would need to disclose for any \textit{direct mechanism} to achieve it, the literature assumes that truthful preference revelation is a sufficient goal in-and-of-itself. The problem here is that the specific preference information any mechanism needs revealed depends on the social choice rule it is attempting to implement, and different factors may influence strategy formation given the various psychological and environmental motivations that create the forms of goal conflict these mechanisms must mediate.

Viewed sympathetically, we can intuit that the field's implicit social choice rule is an "efficient allocation" of blockspace. This seems fair to assume given Roughgarden's own citation of the Vickrey-Clarke-Groves (VCG) mechanism as being UIC and the lack of any seeming challenge to this assumption. And -- to be fair -- the assumption is understandable. If the VCG auction is considered to reveal truthful information sufficient for optimizing participant utility in one context, it does seem intuitive that the same information should be sufficient to optimize utility in a different context. But this intuition is misplaced, as the types of information needed to implement an "efficient allocation" outcome are quite different from that required to calculate "pareto optimality".

Note, for instance, that the VCG auction is a \textit{direct mechanism} that does not require high-dimensional preference information as part of its process of truthful preference revelation. Users share information on the maximum price-point at which they are willing to purchase the single private good being allocated given a fixed price and production schedule for everything else, not their comparative preference for how to divide their resources between all goods and services at all viable price equilibria as required for implementing \textit{pareto optimality}. The VCG auction is thus informationally inadequate for eliminating byzantine strategies motivated by \textit{self-interest} -- our first class of incentive to sub-optimality that emerges when the cost of purchasing utility from the blockchain is higher than the cost of purchasing utility outside the mechanism. Similarly, the VCG auction has no informational basis for combatting \textit{free-riding}, since cooperative strategies to defund public goods cannot be addressed by models that treat blockspace like a private good.

While this secondary problem with public goods is less prevalent in Roughgarden's work, it is a central theme in another influential subset of papers from Elaine Shi and Hao Chung, whose work on \textit{side-contract payments} and the \textit{zero-revenue bound} argue that collusion between users and producers is impossible to disincentivize in any mechanism where the income for block producers is above zero. While these papers disagree on the technical definition of collusion: Shi and Chung suggest the property of "side-contract proof" ("no utility increase from off-chain payments") rather than Roughgarden's more encompassing definition of OCA-Proof ("no utility increase from chain re-organization"), the difference between the two definitions is not germaine to this paper, since \textit{pareto optimality} eliminates both possibilities on the fundamental grounds that in any mechanism that implements it there can be no costless strategy for increasing utility either within an alternate block or across a fork since production is already positioned at the frontier.

While it is somewhat tangential to our discussion of the methodological limitations of auction models, we can observe in passing that the framework of \textit{goal conflict} and particularly \textit{free-riding pressures} provided above provides an intuitive explanation of why Hao and Chung stumble into their zero-revenue bound. As discussed in our previous section, the motivating cause of suboptimal forms of user-producer collusion is the existence of two-sided free-rider problems embedded within \textit{TFM} mechanisms. The \textit{zero-revenue bound} follow deductively from this problem since at any positive fee-level users have an incentive to collude with producers to free-ride on the contributions of their peers to the security budget. This problem can be avoided by compensating producers through an inflationary block reward, but that reverses the problem by inviting producers to collude with users to free-ride on the supply-side payout. Avoiding one trap pushes us into the other, so the only situation in which we avoid collusion completely in their model is if neither fees nor block rewards exist. Non-excludability can only be maintained in mechanisms with free-rider pressures if self-provision is sacrificed.

A more subtle problem that is more general to the auction literature applies to the treatment of block producers, who are simply asked to implement the fee mechanism. From the perspective of mechanism design, the lack of any need for producers to reveal private information raises questions about why we are modelling this game as a two-sided strategic interaction. But the limitation points to a deeper methodological problem connected with the presernce of public goods. For as noted in our first section, the class of \textit{TFMs} we are studying contain dual-sided free-rider problems. This specific class of vulnerability makes it impossible to achieve pareto optimality if we require truthful preference revelation from only one party, for both parties have private incentives to adopt byzantine strategies that are driven by a desire to free-ride on their peers. Eliminating collusion thus requires either eliminating free-riding pressures generally (impossible in auction mechanisms that model blockspace as a private good) or by identifying a kind of "private information" which can by leveraged by a mechanism to motivate producers to shift their strategies away from defunding fection and towards cooperation (impossible in mechanisms that deny producers the strategic agency to act on the basis of private information). Once again, the structural limitations of the ways in which \textit{TFMs} are being modelled precludes the ability to find a solution.

The third and most fundamental problem with the auction model is that it is impossible to generalize its results. For an understanding of the problem here, note that generalization of impossibility results in these papers are typically done by invoking the Revelation Principle and observing that any mechanism capable of achieving a nash equilibrium must have an equivalent in which truthful bidding is a dominant strategy. Leaving aside questions of the informational appropriateness of the model as discussed above, the failure to find an outcome in which truthful bidding is a dominant strategy is then taken as the impossibility of achieving the desired nash equilibrium.

The problem with this argument is that it misapplies the Revelation Principle. Since this is a somewhat subtle point, note that while Maskin teaches us that all nash equilibria which are reachable by \textit{indirect mechanisms} can be implemented as \textit{direct mechanisms}, the opposite is not true.

Understanding this point is important for seeing how the mechanism described later in this paper solves the problem. For Maskin's revelation principle is based on logical reasoning about the consistency of outcomes between decomposable algorithms (where participants compute their preferences privately) and composable algorithms (where users reveal their preferences to a centralized mechanism that does the work for them). In situations where the amount of information required to calculate an optimal solution is so large as to make disclosure impractical or impossible to calculate in a centralized mechanism or bounded messaging channel, such as exists with the high-dimensional preference data needed to compute \textit{pareto optimal} equilibria in informationally decentralized environments, \textit{indirect mechanisms} that use \textit{decomposable algorithms} to filter and transform participant preferences prior to their revelation can be informationally necessary to achieve incentive compatibility. It should be noted that Maskin's revelation principle still holds -- truthful preference revelation happens in both types of mechanism -- but it can happen in a different stage, either in the "action stage" identified by Hurwicz where bids are submitted directly to the market, or obliquely in the "pre-exchange negotiation stage" in a more indirect and filtered form.

The presence of public goods in consensus mechanisms is what forces the need for high-dimensional preference measurement in any composable algorithm (\textit{direct mechanism}) intended to mitigate goal conflict, as their existence pulls utility-optimizing strategies away from maximizing the kind of "single well-defined objective function" Hurwicz associated with computational models and towards the more complicated multi-variate forms of goal conflict handled by economists. Perhaps because of this, it is not surprisingly to see mechanism designers with stronger economic backgrounds explicitly recognize the presence of public goods, as is the case in a recent paper by Elijah Fox, Mallesh Pai, and Max Resnick on "Censorship Resistance in On-Chain Auctions". While the assumptions these authors make are not strictly true -- transactions fees only incentivize the provision of public goods to the extent they induce spending on the security function as the cost of collection and various monetization strategies exist that subvert the need for this even with on-chain payents -- these authors are absolutely correct that off-chain payments involve a form of free-riding and addressing this problem is the key challenge for mechanism designers.

The connection between the presence of public goods and the need to look towards \textit{indirect mechanisms} for any solution is also evident from the way this class of mechanism is the preferred approach even in auction theory for solving these class of problems, such as in the curious case of the Clarke-Groves mechanism (not to be confused with the VCG mechanism), which is an indirect mechanism in which users are asked to submit bids across bundles of goods, some of which may include public goods, and which is leveraged by the mechanism to intuit the comparative shape of the private demand curves and optimize the provision of both private and public goods. Given the parallels between the information requirements to solve both that problem and the one discussed in this paper, it is likely no accident that the solution this paper identifies is an \textit{indirect mechanism} that leverages decomposabitility to avoid the need for truthful preference revelation during the "pre-exchange negotiation step" as a necessary precondition for achieving incentive compatibility.

The dominance of auction-centric analysis in recent academic work on fee mechanisms prompts a general question: what percentage of the remaining papers are writing about fee mechanisms and what percentage are simply writing about auctions? Making similar assumptions as their predecessors (auction model, no clear social choice rule, costless manipulation of informational environment), Aadityan Ganesh, Clayton Thomas and Matthew Wienberg not surprisingly end up in the same place, with the value of their work consisting mostly of new terms like "off-chain influence proofness" to describe specific forms of collusion. Interestingly, by identifying "external opportunities" for profit not captured within the fee mechanism, the authors are returning to questions of overall economic efficiency since their analysis is now explicitly modelling forms of utility which can be captured external to the mechanism. Nonetheless, they fail to follow their observations to their obvious conclusions: that the auction model itself is an inappropriate tool for eliminating these forms of goal conflict. But since the proof-of-stake models they study cannot address any of these problems how could they -- or any of their peers -- be expected to find a solution?

There are nonetheless some positive results that hint at indirect market mechanisms -- not auctions -- hold the key to solving these problems. Rejecting the tendency to treat auctions as one-shot games, \citet{ferreira2021dynamic} observes that abandoning the single-step auction improves outcomes considerably. \citet{chen2022bayesian} likewise proposes a mechanism that relaxes these limitations and identifies \textit{posted-price} mechanisms as a subset of auction designs. From an economic perspective, these papers anticipate the solution offered below by hinting at the benefits of moving pricing information into an environment in which it is costly to manipulate and not subject to byzantine attacks of the sort identified by Hurwicz. Their vulnerability is of course to attacks that attempt to manipulate the information in this environment, which cannot be assumed to be costly in the mechanisms they analyze.

\citet{gafni2024barriers} attempt to characterize deterministic TFMs and show that repeated games are more likely to iterate towards optimal prices. While their mechanism suffers from the same limitations other other approaches in its analysis of a single-block setting, the author's observation that mechanisms with inertia show better performance moves them closer to Hurwicz, who made the same observation in his analysis of the performance of algorithms that attempt to price-optimize generally in both free markets and command economies.

In general, the literature continues to be dominated by technical discussions that frame problems of self-interest and collusion in extremely specific technical contexts, preoccupied largely with claiming new terminology to describe the extremely specific ways that universal \textit{goal conflicts} manifest in variants of direct auction mechanisms, and which do not ask whether any overarching equilibrium exist in which these conflicts might not exist, and what forms of information would need to be revealed or made computable in any fundamental solution.

